{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT: On Colab, we expect your homework to be in the cs189 folder\n",
    "## Please contact staff if you encounter any problems with installing dependencies\n",
    "import sys, os\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/cs189/hw/hw1\n",
    "    %pip install -r ./requirements.txt\n",
    "    !pip install -U kaleido plotly\n",
    "    import kaleido\n",
    "    kaleido.get_chrome_sync()\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = pio.renderers.default + \"+png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"fashion_pt_1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bm6x8k9uPUL5"
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Homework 1.1 – AGI, Everywhere, All at Once</h1>\n",
    "\n",
    "Welcome to Homework 1.1! In this assignment, you will get familiar with common data and visualization tools like `numpy`, `pandas`, and `plotly`. This notebook emphasizes `pandas` operations throughout, and you will work with `DataFrames` as your primary data structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Due Date: Friday, September 19, 11:59 PM\n",
    "\n",
    "This assignment is due on **Friday, September 19, at 11:59 PM**. You must submit your work to Gradescope by this deadline. Please refer to the syllabus for the [Slip Day policy](https://eecs189.org/fa25/syllabus/#slip-days). No late submissions will be accepted beyond the details outlined in the Slip Day policy.\n",
    "\n",
    "### Submission Tips:\n",
    "- **Plan ahead**: We strongly encourage you to submit your work several hours before the deadline. This will give you ample time to address any submission issues.\n",
    "- **Reach out for help early**: If you encounter difficulties, contact course staff well before the deadline. While we are happy to assist with submission issues, we cannot guarantee responses to last-minute requests.\n",
    "\n",
    "---\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "This notebook contains a series of tasks designed to help you practice and apply key concepts in data manipulation and visualization. You will complete all the TODOs in the notebook, which include both coding and written response questions. Some tasks are open-ended, which allows you to explore and experiment with different approaches.\n",
    "\n",
    "### Key Learning Objectives:\n",
    "1. Work with `numpy` and `pandas` for data manipulation.\n",
    "2. Visualize data using `plotly` and `pandas`' built-in plotting functions.\n",
    "3. Gain experience with organizing and analyzing datasets.\n",
    "4. Understand the importance of data exploration and preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "### Grading Breakdown\n",
    "\n",
    "| Question | Manual Grading? | Points |\n",
    "|----------|-----------------|--------|\n",
    "| 0a       | No              | 1      |\n",
    "| 1a       | No              | 1      |\n",
    "| 1b       | No              | 1      |\n",
    "| 1c       | Yes             | 1      |\n",
    "| 1d       | No              | 1      |\n",
    "| 2a       | No              | 2      |\n",
    "| 2b       | No              | 1      |\n",
    "| 2c       | Yes             | 1      |\n",
    "| 2d       | Yes             | 2      |\n",
    "| 3a       | No              | 2      |\n",
    "| 3b       | No              | 2      |\n",
    "| 3c       | No              | 1      |\n",
    "| 3d       | Yes             | 2      |\n",
    "| 3e       | No              | 2      |\n",
    "| 3f       | No              | 1      |\n",
    "| 3g       | No              | 1      |\n",
    "| 3h       | Yes             | 1      |\n",
    "| 3i       | No              | 1      |\n",
    "| 3j       | Yes             | 1      |\n",
    "| 4a       | No              | 1      |\n",
    "| 4b       | No              | 2      |\n",
    "| 4c       | No              | 2      |\n",
    "| 4d       | No              | 2      |\n",
    "| 4e       | No              | 2      |\n",
    "| 4f       | No              | 1      |\n",
    "| 4g       | Yes             | 2      |\n",
    "| 4h       | No              | 1      |\n",
    "| 4i       | No              | 2      |\n",
    "| 4j       | Yes             | 2      |\n",
    "| **Total**|                 | **42** |\n",
    "\n",
    "</div>\n",
    "\n",
    "**Note**: \"Manual\" questions are written response questions that will be graded manually by the course staff. All other questions will be graded automatically by the autograder.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "1. Carefully read each question and its requirements.\n",
    "2. Complete all TODOs in the notebook. You may add extra lines of code if needed to implement your solution.\n",
    "3. For manual questions, provide clear and concise written responses.\n",
    "4. Test your code thoroughly to ensure it meets the requirements.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8E720cxPUL8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torchvision\n",
    "import os\n",
    "import random\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORTANT:** \n",
    "- Do not change the random seed values!!!\n",
    "- Before you submit your notebook, remember to set `save_models=True` and `load_models=True`. This saves your final models which we will use for the autograder. Set these to false if you are still tweaking your model setup. We have provided code for saving models - **do not change these file names!!**\n",
    "- When uploading your notebook, make sure to include your model file `classifier.joblib` in your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducible results\n",
    "SEED = 189\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# IMPORTANT: set save_models to True to save trained models. YOU NEED TO DO THIS FOR THE AUTOGRADER TO WORK.\n",
    "import joblib\n",
    "save_models = True\n",
    "load_saved_models = True # After training, you can set this to True to load the saved models and not have to re-train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdgygg5JPUL9"
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Load the Fashion-MNIST dataset  \n",
    "\n",
    "In this homework, we will work with the Fashion-MNIST dataset, a widely used benchmark dataset for machine learning. It consists of grayscale 28x28 pixel images of various articles of clothing, making it an excellent dataset for practicing image classification.\n",
    "\n",
    ">[Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.](https://arxiv.org/abs/1708.07747) Han Xiao, Kashif Rasul, Roland Vollgraf.\n",
    "> https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "This dataset serves as an alternative to the classic MNIST digits dataset, which contains images of handwritten digits. Fashion-MNIST is more challenging and better reflects real-world image classification tasks.\n",
    "\n",
    "We will load the dataset using [torchvision](https://docs.pytorch.org/vision/stable/index.html), a PyTorch library that provides popular datasets, models, and transformation tools. While you don't need to fully understand PyTorch for this homework, it's helpful to know that the dataset contains two key components: \n",
    "* `data`: the images themselves, represented as 28x28 grayscale arrays.\n",
    "* `targets`: the class labels for each image, where each label corresponds to a specific article of clothing.\n",
    "\n",
    "The dataset includes 10 classes, each representing a type of clothing item:\n",
    "- `T-shirt/top`\n",
    "- `Trouser`\n",
    "- `Pullover`\n",
    "- `Dress`\n",
    "- `Coat`\n",
    "- `Sandal`\n",
    "- `Shirt`\n",
    "- `Sneaker`\n",
    "- `Bag`\n",
    "- `Ankle boot`\n",
    "\n",
    "We will explore this dataset in detail and use it to practice data manipulation, visualization, and machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dlc5yS85QDWj",
    "outputId": "4a18cd56-8f47-4282-e541-66580c888340"
   },
   "outputs": [],
   "source": [
    "# Load the FashionMNIST dataset from torchvision\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\n",
    "\n",
    "# Extract the image data and convert it to a numpy array of type float\n",
    "images = train_data.data.numpy().astype(float)\n",
    "\n",
    "# Extract the target labels as a numpy array\n",
    "targets = train_data.targets.numpy()\n",
    "\n",
    "# Create a dictionary mapping class indices to class names\n",
    "class_dict = {i: class_name for i, class_name in enumerate(train_data.classes)}\n",
    "\n",
    "# Map the target labels to their corresponding class names\n",
    "labels = np.array([class_dict[t] for t in targets])\n",
    "\n",
    "# Create a list of class names in order of their indices\n",
    "class_names = [class_dict[i] for i in range(len(class_dict))]\n",
    "\n",
    "# Get the total number of samples in the dataset\n",
    "n = len(images)\n",
    "\n",
    "# Ensure class_names is a list of class names (redundant but ensures consistency)\n",
    "class_names = list(class_dict.values())\n",
    "\n",
    "# Print dataset information for verification\n",
    "print(\"Loaded FashionMNIST dataset with {} samples.\".format(n))\n",
    "print(\"Classes: {}\".format(class_dict))\n",
    "print(\"Image shape: {}\".format(images[0].shape))  # Shape of a single image\n",
    "print(\"Image dtype: {}\".format(images[0].dtype))  # Data type of the image array\n",
    "print(\"Image type: {}\".format(type(images[0])))   # Type of the image object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's create a `DataFrame` to organize our data**\n",
    "\n",
    "In this class, we will be using a lot of [pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html), which is a powerful library for data analysis and manipulation. A `DataFrame` in pandas is essentially a table where we can store and perform operations on our data.\n",
    "\n",
    "### Why use a `DataFrame`?\n",
    "\n",
    "A `DataFrame` allows us to:\n",
    "- Organize data into rows and columns for better readability.\n",
    "- Perform efficient operations on the data, such as filtering, grouping, and aggregating.\n",
    "- Integrate seamlessly with other libraries for visualization and machine learning.\n",
    "\n",
    "### Problem 0a\n",
    "\n",
    "**Task:** Create a `DataFrame` called `df` with two columns: `image` and `label`. Each row should correspond to an image and its associated label. You can preview the first 5 rows of a `DataFrame` by calling `df.head()`.\n",
    "\n",
    "**Hints:**\n",
    "1. What is the current object type of the variable `images`? Note that `pandas` expects 1D or 2D data for each value in a `DataFrame` column. You may need to first convert `images` to a Python list before using it to create the `DataFrame`.\n",
    "2. Later on, when we use our `DataFrame` for training, it's best if the values in the `image` column are [`ndarray`](https://numpy.org/doc/stable/reference/arrays.ndarray.html) objects. After creating the `DataFrame`, consider re-casting all the values in the `image` column to `ndarray` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create a DataFrame with two columns: `image` and `label`\n",
    "\n",
    "...\n",
    "\n",
    "# Print the shape and columns of the DataFrame\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"DataFrame columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIZz2GUdPUL9"
   },
   "source": [
    "## Problem 1: Introduction to `pandas` and `Plotly`\n",
    "\n",
    "Now that we have created our `DataFrame`, let's start analyzing our data. A key aspect of machine learning is understanding the data you are working with, so let's create some visualizations of our dataset. \n",
    "\n",
    "One of the first steps in data analysis is to check how \"balanced\" the dataset is. This means examining the distribution of the labels to see if each class appears equally in the dataset. A balanced dataset ensures that no class is overrepresented or underrepresented, which can impact the performance of machine learning models.\n",
    "\n",
    "### Problem 1a: Checking Dataset Balance\n",
    "\n",
    "**Task**: Calculate the distribution of the `label` column in the `df` `DataFrame` using [`value_counts()`](https://pandas.pydata.org/pandas-docs/version/2.1.2/reference/api/pandas.Series.value_counts.html) and store it in a variable called `label_distribution`. Then, determine whether or not our dataset is balanced by comparing the minimum and maximum values of `label_distribution`. Store the result as a boolean value in the `is_balanced` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate the distribution of labels using `value_counts()``\n",
    "# TODO: Compare the min and max values of `label_distribution` to determine if the dataset is balanced. \n",
    "\n",
    "...\n",
    "\n",
    "print(f\"Label distribution:\\n{label_distribution}\")\n",
    "print(f\"Is the dataset balanced? {is_balanced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b: Grouping Data with `groupby()`\n",
    "\n",
    "The `groupby()` function in `pandas` is a powerful tool for grouping rows based on column values and applying aggregation functions like `.size()`.\n",
    "\n",
    "**Task:**\n",
    "Group `df` by the `label` column and count the rows in each group using `.size()`.\n",
    "\n",
    "#### Example Output:\n",
    "\n",
    "| label        | count |\n",
    "|--------------|-------|\n",
    "| Ankle boot   | 6000  |\n",
    "| Bag          | 6000  |\n",
    "| ...          | ...   |\n",
    "\n",
    "[Learn more about `groupby()` here.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Group the rows in `df` according to the values in the `labels` column. Then, count the number of rows in each group.\n",
    "label_distribution_groupby = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1c: Visualizing Label Distribution\n",
    "\n",
    "One of the strengths of `pandas` is its ability to quickly generate visualizations of data. This is particularly useful for understanding the distribution of your dataset. In this task, we will use `pandas`' built-in plotting functions to create a visualization of the label distribution in our `DataFrame`.\n",
    "\n",
    "#### Why Visualize Label Distribution?\n",
    "\n",
    "Visualizing the label distribution helps us:\n",
    "- Understand the balance of classes in the dataset.\n",
    "- Identify any potential biases or imbalances that could affect model performance.\n",
    "- Gain insights into the dataset before proceeding with further analysis.\n",
    "\n",
    "**Task:**\n",
    "1. Use the `pandas` [built-in plotting functions](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) to create a histogram of the label distribution. (x-axis is the class label and y axis is the sample count)\n",
    "2. Ensure the chart is clear and labeled appropriately for easy interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Plotting library to use, default is matplotlib but plotly has more functionality\n",
    "pd.options.plotting.backend = \"plotly\" \n",
    "\n",
    "# TODO: Plot a histogram of the labels in the DataFrame `df` using the DataFrame's built-in plotting functions (this should be 1 line)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick refresher, here is the `show_images` function from lecture. This function visualizes our images and labels each of them with what class they are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NLQZ-S3PUL-"
   },
   "outputs": [],
   "source": [
    "def show_images(images, max_images=40, ncols=5, labels = None, reshape=False):\n",
    "    \"\"\"Visualize a subset of images from the dataset.\n",
    "    Args:\n",
    "        images (np.ndarray or list): Array of images to visualize [img,row,col].\n",
    "        max_images (int): Maximum number of images to display.\n",
    "        ncols (int): Number of columns in the grid.\n",
    "        labels (np.ndarray, optional): Labels for the images, used for facet titles.\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: A Plotly figure object containing the images.\n",
    "    \"\"\"\n",
    "    if isinstance(images, list):\n",
    "        images = np.stack(images)\n",
    "    n = min(images.shape[0], max_images) # Number of images to show\n",
    "    px_height = 220 # Height of each image in pixels\n",
    "    if reshape:\n",
    "        images = images.reshape(images.shape[0], 28, 28)\n",
    "    fig = px.imshow(images[:n, :, :], color_continuous_scale='gray_r', \n",
    "                    facet_col = 0, facet_col_wrap=ncols,\n",
    "                    height = px_height * int(np.ceil(n/ncols)))\n",
    "    fig.update_layout(coloraxis_showscale=False)\n",
    "    fig.update_xaxes(showticklabels=False, showgrid=False)\n",
    "    fig.update_yaxes(showticklabels=False, showgrid=False)\n",
    "    if labels is not None:\n",
    "        # Extract the facet number and replace with the label.\n",
    "        fig.for_each_annotation(lambda a: a.update(text=labels[int(a.text.split(\"=\")[-1])]))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "03PCDMLoPUL-",
    "outputId": "54b11705-30ac-473b-a956-4371cd4cd5e1"
   },
   "source": [
    "### Problem 1d: Visualizing Class Examples\n",
    "\n",
    "To better understand the dataset, let's visualize a few examples from each class. This will help us see what the images look like and how they differ across classes.\n",
    "\n",
    "**Task**: \n",
    "1. Use the `pandas` `groupby` function to group the `DataFrame` by the `label` column.\n",
    "2. Sample 2 images per class.\n",
    "3. Use the `show_images` function to display the images in a grid, with each image labeled by its class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "03PCDMLoPUL-",
    "outputId": "54b11705-30ac-473b-a956-4371cd4cd5e1",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Get 2 sample images per class and plot them.\n",
    "examples = ...\n",
    "\n",
    "fig = show_images(examples[\"image\"].tolist(), ncols=4, labels=examples[\"label\"].tolist())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Understanding Data Structure with Clustering\n",
    "\n",
    "Before training classifiers, we explore the data structure using **k-means clustering**, an unsupervised learning method. This helps identify patterns and relationships in the dataset.\n",
    "\n",
    "**Why Clustering?**\n",
    "\n",
    "- **Discover Similarities**: Group similar clothing items based on pixel values.\n",
    "- **Data Insights**: Understand dataset structure to guide modeling.\n",
    "- **Simplify Data**: Potential preprocessing or dimensionality reduction.\n",
    "\n",
    "**Steps:**\n",
    "1. Flatten images for clustering (done below).\n",
    "2. Apply k-means to group images.\n",
    "3. Analyze clusters for patterns.\n",
    "\n",
    "Before we can apply clustering algorithms or train models, we need to preprocess our images. Most machine learning algorithms expect input data to be in a 1-dimensional format. Currently, our images are in a 2D format with dimensions `(28, 28)`. \n",
    "\n",
    "Thus, let's first reshape each image from `(28, 28)` to a 1-dimensional array of size `(784,)` using the Pandas the [`apply()`](https://pandas.pydata.org/pandas-docs/version/2.1.2/reference/api/pandas.DataFrame.apply.html) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"image\"] = df[\"image\"].apply(lambda img: img.reshape(-1))\n",
    "np.stack(df['image'].values).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a: K-means Clustering on the Pixels\n",
    "\n",
    "Use K-means clustering to group similar images based on their pixel values. This will help us understand how well the algorithm can identify patterns in the dataset without using the labels.\n",
    "\n",
    "**Task**: \n",
    "1. Use the [sklearn's `KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class to cluster the images into `10` clusters (since there are 10 classes in the dataset). For efficiency we will only cluster a 1000 image sample (`df_sample`).\n",
    "2. Create a `DataFrame` called `kmeans_df` with the following columns:\n",
    "    - `image`: the image data (flattened to 1D arrays of size 784).\n",
    "    - `label`: the true class label of the image.\n",
    "    - `cluster`: the cluster label assigned by K-means.\n",
    "\n",
    "**Instructions**:\n",
    "- When clustering, set `random_state=SEED` for reproducibility.\n",
    "\n",
    "**Expected Output**:\n",
    "The `kmeans_df` DataFrame should look like this:\n",
    "\n",
    "| cluster | label       | image                                   |\n",
    "|---------|-------------|-----------------------------------------|\n",
    "| 7       | Ankle boot  | [0.0, 0.0, 0.0, 0.0, 0.0, ...]         |\n",
    "| 6       | T-shirt/top | [0.0, 0.0, 0.0, 0.0, 1.0, ...]         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Perform k-means clustering on the images (10 clusters to match the number of classes)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df_sample = df.sample(n=1000, random_state=SEED)\n",
    "...\n",
    "\n",
    "kmeans_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: Evaluating K-means Clustering\n",
    "\n",
    "K-means clustering groups data points into clusters based on their similarity. To evaluate how well the clustering algorithm has separated the classes, we can analyze the distribution of true labels within each cluster.\n",
    "\n",
    "**Task:** \n",
    "1. Use the `kmeans_df` `DataFrame` to calculate the distribution of true labels (`label`) within each cluster (`cluster`).\n",
    "2. Create a [stacked bar plot](https://plotly.com/python/bar-charts/) to visualize the label counts per cluster. Each bar should represent a cluster, and the segments of the bar should represent the counts of each label within that cluster.\n",
    "\n",
    "**Hint:** If you are running into issues where there are bars “hidden” behind other ones in your Plotly bar chart, try making sure you use fillna(0) or unstack(fill_value=0) after grouping by your KMean clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create a stacked bar plot of the label counts per cluster.\n",
    "cluster_label_counts = ...\n",
    "\n",
    "cluster_label_counts.plot(\n",
    "    kind='bar',\n",
    "    title='Distribution of True Labels in Each K-means Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c: Visualizing Clusters\n",
    "\n",
    "To better understand the clusters formed by the K-means algorithm, we will visualize a few sample images from each cluster. This will help us identify patterns or similarities among images within the same cluster.\n",
    "\n",
    "**Task:**\n",
    "1. For each cluster, randomly sample 7 images.\n",
    "2. Use the `show_images` function to display the sampled images in a grid.\n",
    "3. Observe the visual similarities among images in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Plot 7 images from each cluster (use the show_images function, 10 rows, 7 columns)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Problem 2d: Observing Patterns in K-means Clustering\n",
    "\n",
    "Reflecting on the visualizations from the previous part, we observe that the k-means clustering algorithm groups images not only by their clothing category (class) but also by other shared characteristics. \n",
    "\n",
    "**Question:** Besides the clothing category, what other visual or structural characteristics of the images might the k-means clustering algorithm be grouping together? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Problem 3: Training a Classifier\n",
    "\n",
    "In this section, we will train a machine learning classifier to predict clothing categories from image pixel data. Specifically, we will use a Multi-Layer Perceptron (MLP) classifier, which is a type of neural network.\n",
    "\n",
    "### Workflow Overview\n",
    "\n",
    "We will follow a structured workflow:\n",
    "1. **Data Preparation**: Split the dataset into training and testing sets while maintaining class balance.\n",
    "2. **Model Training**: Train the MLP classifier on the training set.\n",
    "3. **Model Evaluation**: Evaluate the classifier's performance on the test set using metrics like accuracy.\n",
    "4. **Visualization**: Visualize predictions and analyze misclassifications to understand model behavior.\n",
    "\n",
    "This workflow mirrors the process used in the lecture notebook, but you will implement some of the functions yourself to deepen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Train/Test Split** As mentioned in lecture, first we will split our dataset into training and testing sets. This is a crucial step in machine learning to evaluate how well a model generalizes to unseen data. \n",
    "\n",
    "Unlike the lecture, where we used sklearn's [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function, we have split our dataset using pandas functions. \n",
    "\n",
    "**Do not change this function!** Otherwise the autograder will likely fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "train_df = df_copy.groupby('label').sample(frac=0.8, random_state=SEED)\n",
    "test_df = df_copy[~df_copy.index.isin(train_df.index)]\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3a: Train MLP Classifier\n",
    "\n",
    "In this task, we will train a Multi-Layer Perceptron (MLP) classifier to predict clothing categories from image data. The MLP is a type of neural network that is well-suited for classification tasks. The demo notebook from lecture 3 could be particularly useful.  \n",
    "\n",
    "**Steps to Follow:**\n",
    "\n",
    "1. **Data Normalization**:\n",
    "    - Scale the pixel values of the images to the range `[0, 1]` for better training performance.\n",
    "    - Create new variables `X_train_sc` and `X_test_sc` for the scaled training and testing data, respectively. Do not overwrite the original `X_train` and `X_test`.\n",
    "\n",
    "2. **Model Training**:\n",
    "    - Use the same MLP configuration (size, hyperparameters) as demonstrated in the lecture 3 notebook.\n",
    "    - Train the model on the normalized training data.\n",
    "\n",
    "3. **Loss Curve**:\n",
    "    - Extract the loss curve from the trained model using the `model.loss_curve_` attribute.\n",
    "    - Create a `DataFrame` called `loss_df` with two columns: `epoch` and `loss`.\n",
    "    - Use Plotly Express to plot the loss curve, showing how the loss decreases as the number of epochs increases.\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- The term \"loss\" refers to the error (textbook terminology) during training. Minimizing the loss is the goal of the training process.\n",
    "- Ensure that the model is trained with reproducibility in mind (e.g., set a random seed to `SEED` where applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Importing necessary modules for training and preprocessing\n",
    "from sklearn.neural_network import MLPClassifier  # Multi-Layer Perceptron Classifier for training\n",
    "from sklearn.preprocessing import StandardScaler  # StandardScaler for normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# flatten features into 1D arrays\n",
    "X_train = np.stack(train_df['image'].values)\n",
    "y_train = train_df['label'].values\n",
    "X_test = np.stack(test_df['image'].values)\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\\t y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\\t y_test shape: {y_test.shape}\")\n",
    "\n",
    "# TODO: Train the model using the scaled traning data and plott the loss curve (remeber to normalize your data!)\n",
    "# NOTE: Your model must be named `model`\n",
    "\n",
    "if load_saved_models and os.path.exists('classifier.joblib'):\n",
    "    model = joblib.load('classifier.joblib')\n",
    "    ...\n",
    "else:\n",
    "    ...\n",
    "if save_models:\n",
    "    joblib.dump(model, 'classifier.joblib')\n",
    "\n",
    "loss_df.plot(x='epoch', y='loss', title=\"Training Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3b: Adding Predictions and Evaluation Metrics to DataFrames\n",
    "\n",
    "**Task:** Modify both `train_df` and `test_df` by adding the following columns and compute train and test accuracy:\n",
    "\n",
    "1. **`predicted_label`**: The predicted label for each image, as determined by the trained model.\n",
    "2. **`correct`**: A boolean value indicating whether the predicted label matches the true label (`True` for correct predictions, `False` otherwise).\n",
    "3. **`probs`**: The class probabilities for each image, represented as a list of size 10 (one probability per class).\n",
    "4. **`confidence`**: The probability associated with the predicted label, representing the model's confidence in its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Add the columns listed above to `train_df` and `test_df`.\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "...\n",
    "\n",
    "print(\"--- Column Types ----\")\n",
    "for col in train_df.columns:\n",
    "    val = train_df[col].iloc[0]\n",
    "    print(f\"{col}: {type(val)}\")\n",
    "print(\"-----------\")\n",
    "\n",
    "train_accuracy = ...\n",
    "test_accuracy = ...\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3c: Class Accuracy Analysis and Visualization\n",
    "\n",
    "Analyze the model's performance for each class and visualize the class-wise accuracy for both the training and testing datasets.\n",
    "\n",
    "#### **Task 1**: Create a `class_accuracy` DataFrame\n",
    "1. Group the `train_df` and `test_df` DataFrames by `label` (class).\n",
    "2. Calculate the accuracy for each class as the proportion of correct predictions (`correct` column).\n",
    "3. Add a `split` column to indicate whether the data is from the training or testing set.\n",
    "4. Combine the results into a single DataFrame called `class_accuracy` with the following columns:\n",
    "    - `split`: Indicates whether the data is from the training or testing set.\n",
    "    - `label`: The class label.\n",
    "    - `correct`: The accuracy for the class.\n",
    "\n",
    "\n",
    "#### **Task 2**: Visualize Class Accuracy\n",
    "1. Use the `class_accuracy` DataFrame to create a grouped bar chart.\n",
    "2. The x-axis should represent the class labels (`label`), and the y-axis should represent the accuracy (`correct`).\n",
    "3. Use different colors for the training and testing splits:\n",
    "    - Training: Blue\n",
    "    - Testing: Red\n",
    "4. Add the actual accuracy values on top of the bars, rounded to two decimal places. To do this you can add `text_auto=True` to your `.plot` call. If you want to round these numbers to the nearest 2nd decimal, set `text_auto='.2f'`\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "- Use `reset_index()` after grouping to convert the grouped data into a DataFrame.\n",
    "\n",
    "For example, after a groupby:\n",
    "\n",
    "    df.groupby(['A', 'B'])['C'].mean()\n",
    "\n",
    "you get a Series with a multi-index:\n",
    "\n",
    "```bash\n",
    "    A      B       \n",
    "    foo    x      0.92\n",
    "           y      0.85\n",
    "    bar    x      0.99\n",
    "           y      0.97\n",
    "    Name: C, dtype: float64\n",
    "```\n",
    "\n",
    "If you call `.reset_index()`, you get a `DataFrame` with columns:\n",
    "\n",
    "```bash\n",
    "       A    B     C\n",
    "    0  foo  x  0.92\n",
    "    1  foo  y  0.85\n",
    "    2  bar  x  0.99\n",
    "    3  bar  y  0.97\n",
    "```\n",
    "\n",
    "This makes it much easier to plot or further manipulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate train and test accuracy per class \n",
    "# TODO: Use class_accuracy to create a grouped bar chart of class accuracy for train and test\n",
    "\n",
    "...\n",
    "print(class_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3d: Best and Worst Performing Classes\n",
    "\n",
    "**Question:**  \n",
    "- Identify the best and worst performing classes for train and test splits. If tied, list all classes with the same performance.  \n",
    "- Do the best/worst performing classes match between splits?  \n",
    "- Do train and test accuracies differ? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Problem 3e: Create Confusion Matrix\n",
    "\n",
    "An often easier way to understand model performance is with a confuction matrix, which show how often predictions match the true labels and where errors occur. \n",
    "\n",
    "---\n",
    "\n",
    "#### Refresher:\n",
    "1. **Precision**: Measures the accuracy of positive predictions for a class.\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "\n",
    "2. **Recall**: Measures the ability to identify all positive samples for a class.\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**Tasks**:\n",
    "1. **Hand-implement a confusion matrix**:\n",
    "  - Use numpy operations to compute a 10x10 matrix where rows represent true labels and columns represent predicted labels.\n",
    "  \n",
    "2. **Visualize the confusion matrix**:\n",
    "  - Use a [heatmap](https://plotly.com/python/heatmaps/) to display the matrix for better interpretability. Y axis should be the true label and the X axis should be the predicted label. \n",
    "\n",
    "3. **Using your confusion matrix, evaluate performance**:\n",
    "  - Compute overall test accuracy.\n",
    "  - Calculate precision and recall for each class using the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize confusion matrix with zeros\n",
    "conf_matrix = np.zeros((len(class_names), len(class_names)), dtype=int)\n",
    "class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# Fill the confusion matrix by counting predictions and plot it as a heatmap\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy from confusion matrix\n",
    "accuracy_from_matrix = ...\n",
    "print(f\"\\nAccuracy calculated from confusion matrix: {accuracy_from_matrix:.3f}\")\n",
    "\n",
    "# Calculate per-class metrics from confusion matrix\n",
    "per_class_metrics = []\n",
    "print(\"\\nPer-class metrics from confusion matrix:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    true_positives = ...\n",
    "    false_positives = ...\n",
    "    false_negatives = ...\n",
    "    per_class_metrics.append({\n",
    "        'class': class_name,\n",
    "        ...\n",
    "        ...\n",
    "    })\n",
    "    \n",
    "pd.DataFrame(per_class_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3f: Analyze Prediction Confidence\n",
    "\n",
    "In this section, we will analyze the model's prediction confidence to better understand its behavior. Specifically, we will identify examples where the model is uncertain or overly confident, and evaluate how these cases relate to the correctness of its predictions.\n",
    "\n",
    "#### Objectives:\n",
    "1. **Find the Image with the Lowest Confidence**:\n",
    "    - Identify the image for which the model has the least confidence in its prediction.\n",
    "\n",
    "2. **Analyze Low Confidence but Correct Predictions**:\n",
    "    - Find examples where the model made the correct prediction but with low confidence.\n",
    "\n",
    "3. **Analyze High Confidence but Incorrect Predictions**:\n",
    "    - Identify examples where the model is highly confident but makes incorrect predictions.\n",
    "\n",
    "**Task:** Let’s start by finding **the image with the lowest confidence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Find the image with the lowest confidence by sorting the `confidence` column of `test_df`\n",
    "least_confident = ...\n",
    "print(\"Image with lowest confidence:\")\n",
    "print(least_confident[['label', 'predicted_label', 'confidence', 'correct']][:3])\n",
    "\n",
    "# Show image with lowest confidence and its predicted label\n",
    "show_labels = [f\"{label} (Pred: {predicted_label})\" for label, predicted_label in zip(least_confident[\"label\"].tolist(), least_confident[\"predicted_label\"].tolist())]\n",
    "fig = show_images(np.stack(least_confident[\"image\"].tolist()), 8, ncols=4, labels=show_labels, reshape=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3g: Investigating Class Confusion for \"Ankle boot\"\n",
    "\n",
    "**Task:**\n",
    "Visualize Low-Confidence Correct Predictions: Display 10 test images where the true label is \"Ankle boot,\" the prediction is correct, but confidence is lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Visualize 10 images from the `test_set` whose true label is `Ankle boot` that the model correctly classified but with low confidence\n",
    "test_df_boot = ...\n",
    "\n",
    "# Find low confidence correct predictions (uncertain but right)\n",
    "low_conf_correct = ...\n",
    "\n",
    "# Visualize low confidence correct predictions\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3h: Reasons for Low Confidence in the \"Ankle boot\" Class\n",
    "\n",
    "**Task:** Analyze visual patterns in low-confidence images for the \"Ankle boot\" class. What is a potential reasons for the model to be so unconfident in these classifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Problem 3i: Investigating Class Confusion for \"Trouser\"\n",
    "\n",
    "Now let's look at cases where the model is confidently incorrect. \n",
    "\n",
    "**Task:** For the `Trouser` class, visualize the 10 images from the test set which are incorrectly classified as `Dress` but have the highest confience and answer the question below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Visualize 10 images from the `test_set` whose true label is `Trouser` that the model incorrectly classified as `Dress` with high confidence\n",
    "test_df_trouser = ...\n",
    "\n",
    "# Find high confidence incorrect predictions (confident but() wrong)\n",
    "high_conf_incorrect = ...\n",
    "\n",
    "# Visualize high confidence incorrect predictions\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3j: Reasons for High Confidence in the \"Trouser\" Class\n",
    "\n",
    "**Task:** What are some potential reasons for the model to be so confident in its classifications of some of these examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Now that we have become more familiar with the modeling process, let’s look at how we can augment our data and how these augmentations affect our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Image Augmentation via Transformation Matrices\n",
    "\n",
    "In this problem, you will explore how to implement image augmentations such as rotation, flipping, and scaling—using **matrix multiplication**. The goal is to construct a transformation matrix $T$ such that, when multiplied by a flattened image vector, it produces the augmented image:\n",
    "\n",
    "$$\\text{augmented\\_image} = T \\cdot \\text{original\\_image} = \\text{original\\_image} \\cdot T^T$$\n",
    "\n",
    "Each transformation matrix $T$ will be of size $N \\times N$, where $N$ is the total number of pixels in the image (e.g., for a 28×28 image, $N=784$). Each row of $T$ defines how to compute the value of a single output pixel as a weighted sum of the input pixels.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use a Transformation Matrix?\n",
    "\n",
    "Using a matrix for image transformations has several advantages:\n",
    "1. **Efficiency**: Matrix multiplication is computationally efficient and can be optimized for hardware acceleration.\n",
    "2. **Composability**: Multiple transformations (e.g., rotation followed by scaling) can be combined into a single matrix by multiplying their respective transformation matrices.\n",
    "3. **Flexibility**: Any linear transformation, including interpolation, can be represented as a matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Horizontal Flip Matrix\n",
    "\n",
    "Let’s consider a simple example of flipping a 3×3 image horizontally. The flattened image is ordered row-wise:\n",
    "\n",
    "Original indices:\n",
    "$$\\begin{bmatrix}\n",
    "0 & 1 & 2 \\\\\n",
    "3 & 4 & 5 \\\\\n",
    "6 & 7 & 8\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "After a horizontal flip, the columns are reversed:\n",
    "$$\\begin{bmatrix}\n",
    "2 & 1 & 0 \\\\\n",
    "5 & 4 & 3 \\\\\n",
    "8 & 7 & 6\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The transformation matrix $T$ for this operation is a **permutation matrix** that swaps the columns for each row. For a 3×3 image, $T$ is a 9×9 matrix where each row has a single 1 in the position corresponding to the flipped pixel, and 0 elsewhere.\n",
    "\n",
    "---\n",
    "\n",
    "In this question, you will: \n",
    "\n",
    "1. **Understand Transformation Matrices**:\n",
    "    - Learn how to construct transformation matrices for common operations like shifting, blurring, and rotating.\n",
    "\n",
    "2. **Implement Augmentations**:\n",
    "    - Write code to generate transformation matrices for the following operations:\n",
    "      - **Shifting**: Move the image left, right, up, or down.\n",
    "      - **Blurring**: Apply a smoothing effect by averaging neighboring pixels.\n",
    "      - **Rotating**: Rotate the image by a specified angle.\n",
    "\n",
    "3. **Combine Transformations**:\n",
    "    - Experiment with combining multiple transformations into a single matrix and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method will consist of two steps:\n",
    "\n",
    "1. **Create the Transformation Matrix**:  \n",
    "    Construct a 784x784 transformation matrix that represents the desired image augmentation (e.g., rotation, flipping, scaling). Each row of the matrix determines how the value of a single output pixel is computed as a weighted sum of the input pixels.\n",
    "\n",
    "2. **Apply the Transformation**:  \n",
    "    Use the `apply_transformation` function (provided below) to apply the transformation matrix to your image. This function will handle the matrix multiplication and reshape the output back into the original image dimensions.\n",
    "\n",
    "**Example: Vertical Flip**\n",
    "To help you get started, we have implemented a simple vertical flip as an example. This transformation matrix swaps the rows of the image, flipping it vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(image, T):\n",
    "    # Input: A (N, 784) image vector and a (784, 784) transformation matrix\n",
    "    # Output: A (N, 784) image vector\n",
    "    transformed_flat = image @ T.T\n",
    "    return transformed_flat.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vertical_flip_matrix(height=28, width=28):\n",
    "    \"\"\"\n",
    "    Returns a (height*width, height*width) matrix that vertically flips an image\n",
    "    when applied to its flattened vector. Values are 0 or 1.\n",
    "    \"\"\"\n",
    "    N = height * width  # Total number of pixels in the image\n",
    "    T = np.zeros((N, N), dtype=int)  # Initialize the transformation matrix with zeros\n",
    "    for i in range(height):  # Loop over each row\n",
    "        for j in range(width):  # Loop over each column\n",
    "            orig_idx = i * width + j  # Compute the flattened index for the original pixel\n",
    "            flipped_i = height - 1 - i  # Compute the row index after vertical flip\n",
    "            flipped_idx = flipped_i * width + j  # Compute the flattened index for the flipped pixel\n",
    "            # Set the corresponding entry in the transformation matrix to 1\n",
    "            # This means the pixel at (i, j) moves to (flipped_i, j)\n",
    "            T[flipped_idx, orig_idx] = 1\n",
    "    return T\n",
    "\n",
    "def vertical_flip(image):\n",
    "    T_flip = create_vertical_flip_matrix()\n",
    "    return apply_transformation(image, T_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.load(\"test_image.npy\")\n",
    "\n",
    "flipped_image = vertical_flip(test_image)\n",
    "show_images(np.stack([test_image, flipped_image]), labels=['Original', 'Flipped'], reshape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4a: Horizontal Flip\n",
    "\n",
    "Now, let's implement a horizontal flip transformation using a matrix. A horizontal flip mirrors the image along its vertical axis. For example, the leftmost column becomes the rightmost column.\n",
    "\n",
    "**Steps**:\n",
    "1. **Understand the Transformation Matrix**:\n",
    "    - The matrix `T` is `N x N` (where `N = height * width`).\n",
    "    - Each row of `T` has a single `1` to indicate the new position of a pixel after the flip.\n",
    "\n",
    "2. **Construct the Matrix**:\n",
    "    - For each pixel `(i, j)`, compute its new position `(i, width - 1 - j)`.\n",
    "\n",
    "3. **Apply the Transformation**:\n",
    "    - Use the `apply_transformation` function to apply `T` to the flattened image.\n",
    "\n",
    "**Hints**:\n",
    "- Adjust the `flipped_j` and `flipped_idx` variables for the horizontal flip.\n",
    "- Ensure the function returns a **flattened** image after applying the transformation.\n",
    "- Fill any empty spaces in the transformed image with `0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_horizontal_flip_matrix(height=28, width=28):\n",
    "    \"\"\"\n",
    "    Returns a (height*width, height*width) matrix that horizontally flips an image\n",
    "    when applied to its flattened vector. Values are 0 or 1.\n",
    "    \"\"\"\n",
    "    N = height * width\n",
    "    T = np.zeros((N, N), dtype=int)\n",
    "    ...\n",
    "    return T\n",
    "\n",
    "def horizontal_flip(image):\n",
    "    T_flip = create_horizontal_flip_matrix()\n",
    "    return apply_transformation(image, T_flip)\n",
    "\n",
    "flipped_image = horizontal_flip(test_image)\n",
    "\n",
    "show_images(np.stack([test_image, flipped_image]), labels=['Original', 'Horizontal Flipped'], reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4b: Image Shifting\n",
    "\n",
    "**Task**: Implement a function to shift images by a specified number of pixels in any direction.\n",
    "\n",
    "**Steps**:  \n",
    "- Create a function that shifts an image by `dx` pixels horizontally and `dy` pixels vertically.  \n",
    "- Fill empty spaces with 0s.  \n",
    "- Handle cases where the shift moves parts of the image outside the boundaries.  \n",
    "- Return the shifted image as a flattened array.\n",
    "\n",
    "**Hint**:  \n",
    "Think of copying pixels from a source region in the original image to a destination region in the final image. For example:  \n",
    "- If `dx` is positive (shift right), the source x-range starts at 0 and ends at `28 - dx`.  \n",
    "- If `dx` is negative (shift left), the source x-range starts at `-dx` and ends at 28.   \n",
    "- If `dy` is positive (shift up), the source y-range starts at 0 and ends at `28 - dy`.  \n",
    "- If `dy` is negative (shift down), the source y-range starts at `-dy` and ends at 28.  \n",
    "\n",
    "Ensure the function returns a **flattened** image.\n",
    "\n",
    "Fill any empty spaces in the transformed image with `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_shift_matrix(dx, dy, height=28, width=28):\n",
    "    \"\"\"\n",
    "    Create a transformation matrix for shifting an image by dx pixels horizontally and dy pixels vertically.\n",
    "\n",
    "    Args:\n",
    "        dx (int): Number of pixels to shift horizontally.\n",
    "        dy (int): Number of pixels to shift vertically.\n",
    "        height (int): Height of the image.\n",
    "        width (int): Width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (height*width, height*width) transformation matrix for shifting.\n",
    "    \"\"\"\n",
    "    N = height * width\n",
    "    T = np.zeros((N, N))\n",
    "    ...\n",
    "    return T\n",
    "\n",
    "\n",
    "def shift_image(image, dx, dy):\n",
    "    \"\"\"\n",
    "    Shift an image by dx pixels horizontally and dy pixels vertically.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Flattened image array of shape (height*width,).\n",
    "        dx (int): Number of pixels to shift horizontally.\n",
    "        dy (int): Number of pixels to shift vertically.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Shifted image as a flattened array.\n",
    "    \"\"\"\n",
    "    T = create_shift_matrix(dx, dy)\n",
    "    return apply_transformation(image, T)\n",
    "\n",
    "shifted_right_image = shift_image(test_image, 5, 0)\n",
    "shifted_left_image = shift_image(test_image, -5, 0)\n",
    "shifted_up_image = shift_image(test_image, 0, -5)\n",
    "shifted_down_image = shift_image(test_image, 0, 5)\n",
    "\n",
    "all_images = np.stack([test_image, shifted_up_image, shifted_down_image, shifted_right_image, shifted_left_image])\n",
    "plot_labels = ['Original', 'Shifted Up', 'Shifted Down', 'Shifted Right', 'Shifted Left']\n",
    "show_images(all_images, labels=plot_labels, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4c: Image Blurring\n",
    "\n",
    "**Task**  \n",
    "Implement a blurring function using a transformation matrix that averages the values of neighboring pixels.\n",
    "\n",
    "---\n",
    "\n",
    "**What is blurring?**  \n",
    "Blurring reduces the sharpness of an image by averaging each pixel with its neighbors, creating a smoother appearance.  \n",
    "This is done with a **sliding square kernel** (window) that moves across the image.  \n",
    "For each pixel, the kernel specifies which surrounding pixels contribute to the average.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Concepts**\n",
    "- **Kernel Size**  \n",
    "  Controls how many neighbors are included in the average.\n",
    "  - A **3×3 kernel** averages a pixel with its 8 immediate neighbors.\n",
    "  - A **5×5 kernel** averages a pixel with its 24 neighbors.\n",
    "- **Blurring Process**\n",
    "  1. For each pixel, place a square kernel centered on that pixel.\n",
    "  2. Collect all pixels that fall inside the kernel and inside the image.\n",
    "  3. Compute the average of these valid pixels and assign it to the center pixel.\n",
    "\n",
    "> *Edge handling:*  \n",
    "> If the kernel extends beyond the image border, only the pixels that actually overlap the image are averaged.\n",
    "\n",
    "---\n",
    "\n",
    "**Example with a 4×4 image using a 3×3 kernel**\n",
    "\n",
    "Original 4×4 image:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "10 & 20 & 30 & 40 \\\\\n",
    "15 & 25 & 35 & 45 \\\\\n",
    "50 & 60 & 70 & 80 \\\\\n",
    "55 & 65 & 75 & 85\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "Consider the pixel with value 25 in row 2 col 2 [index (1, 1) in the matrix].  \n",
    "Its 3×3 window contains:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "10 & 20 & 30 \\\\\n",
    "15 & \\textbf{25} & 35 \\\\\n",
    "50 & 60 & 70\n",
    "\\end{bmatrix}\n",
    "\n",
    "The blurred value for this position is the average of the numbers in this window (35).\n",
    "\n",
    "For a corner pixel like (0, 0), the 3×3 window lies partly outside the image, so we average only the valid four neighbors:\n",
    "\\[\n",
    "\\frac{10 + 20 + 15 + 25}{4} = 17.5.\n",
    "\\]\n",
    "\n",
    "Applying this process to every pixel produces a softened 4×4 image.\n",
    "\n",
    "---\n",
    "\n",
    "**Steps:**\n",
    "1. Implement a function that, for each pixel, averages over a centered square window (kernel) of odd size (e.g., 3, 5, 7).  \n",
    "   Handle edges by averaging only the valid neighbors.\n",
    "2. Use a transformation matrix to apply this operation to the entire image.\n",
    "3. Ensure the function works for any odd kernel size (e.g., 3x3, 5x5).\n",
    "4. Return the blurred image as a flattened array.\n",
    "\n",
    "\n",
    "Fill any empty spaces in the transformed image with `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_blur_matrix(kernel_size=3, height=28, width=28):\n",
    "    \"\"\"\n",
    "    Create a transformation matrix T that applies a uniform mean blur using a centered, odd-sized square sliding window.\n",
    "\n",
    "    For each output pixel (i, j):\n",
    "      1) Place a `kernel_size × kernel_size` window centered at (i, j).\n",
    "      2) If the window is outside the image, then it will have fewer neighbors (only average the pixels that exist)\n",
    "\n",
    "    Args:\n",
    "        kernel_size (int): Size of the square kernel (must be odd).\n",
    "        height (int): Height of the image.\n",
    "        width (int): Width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (height*width, height*width) transformation matrix for blurring.\n",
    "    \"\"\"\n",
    "    N = height * width\n",
    "    T = np.zeros((N, N))\n",
    "    pad = kernel_size // 2\n",
    "    kernel_area = kernel_size * kernel_size\n",
    "    # Each pixel contributes equally to a kernel's neighborhood\n",
    "    ...\n",
    "    return T\n",
    "\n",
    "def blur_image(image, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Apply a blur transformation to a flattened image array or a batch of flattened images.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Flattened image array of shape (height*width,) or batch of images (N, height*width).\n",
    "        kernel_size (int): Size of the square kernel to use for blurring.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Blurred image(s) as a flattened array or batch of arrays.\n",
    "    \"\"\"\n",
    "    T = create_blur_matrix(kernel_size)\n",
    "    return apply_transformation(image, T)\n",
    "\n",
    "blurred_1x1 = blur_image(test_image, kernel_size=1)\n",
    "blurred_3x3 = blur_image(test_image, kernel_size=3)\n",
    "blurred_5x5 = blur_image(test_image, kernel_size=5)\n",
    "\n",
    "blurred_images = [test_image, blurred_1x1, blurred_3x3, blurred_5x5]\n",
    "blurred_labels = ['Original', 'Blur 1x1', 'Blur 3x3', 'Blur 5x5']\n",
    "\n",
    "show_images(blurred_images, labels=blurred_labels, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Problem 4d: Image Rotation\n",
    "\n",
    "**Task**: Implement a function to rotate an image by a given angle `theta` (in degrees).\n",
    "\n",
    "**Steps**:\n",
    "1. **Create the Rotation Matrix**:\n",
    "    - Write a function `create_rotation_matrix(theta)` that generates a transformation matrix to rotate a flattened image by `theta` degrees.\n",
    "    - Convert `theta` from degrees to radians using `np.deg2rad(theta)` before applying trigonometric functions.\n",
    "    - Ensure the center of rotation is the center of the image.\n",
    "\n",
    "2. **Apply the Transformation**:\n",
    "    - The output should be a transformation matrix of shape `(height*width, height*width)`.\n",
    "    - When this matrix is multiplied by the flattened image, it should produce the rotated image (also flattened).\n",
    "\n",
    "**Hint**: Use trigonometric functions (`sin`, `cos`) to calculate the new positions of pixels after rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_rotation_matrix(theta, height=28, width=28):\n",
    "    \"\"\"\n",
    "    Create a transformation matrix for rotating an image by theta degrees.\n",
    "\n",
    "    Args:\n",
    "        theta (float): Angle of rotation in degrees.\n",
    "        height (int): Height of the image.\n",
    "        width (int): Width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (height*width, height*width) transformation matrix for rotating.\n",
    "    \"\"\"\n",
    "    # Convert theta from degrees to radians\n",
    "    theta = np.deg2rad(theta)\n",
    "    N = height * width\n",
    "    T = np.zeros((N, N))\n",
    "    center_i = (height - 1) / 2.0\n",
    "    center_j = (width - 1) / 2.0\n",
    "    ...\n",
    "    return T\n",
    "\n",
    "\n",
    "def rotate_image(image, theta):\n",
    "    \"\"\"\n",
    "    Apply a rotation transformation to a flattened image array or a batch of flattened images.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Flattened image array of shape (height*width,) or batch of images (N, height*width).\n",
    "        theta (float): Angle of rotation in degrees.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Rotated image(s) as a flattened array or batch of arrays.\n",
    "    \"\"\"\n",
    "    T = create_rotation_matrix(theta)\n",
    "    return apply_transformation(image, T)\n",
    "\n",
    "# rotate with matrix\n",
    "rotated_45 = rotate_image(test_image, 45) \n",
    "rotated_90 = rotate_image(test_image, 90)\n",
    "rotated_200 = rotate_image(test_image, 200)\n",
    "rotated_270 = rotate_image(test_image, 270)\n",
    "\n",
    "# visualize original and 4 augmentations in plotly image grid\n",
    "all_images = np.stack([test_image, rotated_45, rotated_90, rotated_200, rotated_270])\n",
    "plot_labels = ['Original', 'Rotated (45°)', 'Rotated (90°)', 'Rotated (200°)', 'Rotated (270°)']\n",
    "show_images(all_images, labels=plot_labels, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice something?** For some rotations, we are left with holes in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Gaps in Rotated Images\n",
    "\n",
    "When rotating an image, you may notice white spaces (gaps) in the output. These gaps occur due to the way nearest-neighbor interpolation works. Let’s explore this using a simple $3 \\times 3$ image.\n",
    "\n",
    "---\n",
    "\n",
    "**Original Image Grid**\n",
    "\n",
    "The pixel coordinates are:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "(0,0) & (0,1) & (0,2) \\\\\n",
    "(1,0) & (1,1) & (1,2) \\\\\n",
    "(2,0) & (2,1) & (2,2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The center of the image is at $(1,1)$.\n",
    "\n",
    "---\n",
    "\n",
    "**Rotation by $45^\\circ$**\n",
    "\n",
    "1. **Translate the center to the origin**  \n",
    "   For pixel $(0,0)$:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
    "   =\n",
    "   \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
    "   -\n",
    "   \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "   =\n",
    "   \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Apply the rotation matrix**  \n",
    "   The rotation matrix for $45^\\circ$ is:\n",
    "\n",
    "   $$\n",
    "   R(45^\\circ) =\n",
    "   \\tfrac{1}{\\sqrt{2}}\n",
    "   \\begin{bmatrix}\n",
    "   1 & -1 \\\\\n",
    "   1 & 1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   Applying the rotation:\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix} x' \\\\ y' \\end{bmatrix}\n",
    "   =\n",
    "   R(45^\\circ)\n",
    "   \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}\n",
    "   =\n",
    "   \\tfrac{1}{\\sqrt{2}}\n",
    "   \\begin{bmatrix} (-1) - (-1) \\\\ (-1) + (-1) \\end{bmatrix}\n",
    "   =\n",
    "   \\begin{bmatrix} 0 \\\\ -\\sqrt{2} \\end{bmatrix}\n",
    "   \\approx\n",
    "   \\begin{bmatrix} 0 \\\\ -1.4142 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Translate back to the original center**\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix} \\text{new}_x \\\\ \\text{new}_y \\end{bmatrix}\n",
    "   =\n",
    "   \\begin{bmatrix} 0 \\\\ -1.4142 \\end{bmatrix}\n",
    "   +\n",
    "   \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "   \\approx\n",
    "   \\begin{bmatrix} 1 \\\\ -0.4142 \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Nearest-Neighbor Assignment**\n",
    "\n",
    "To map the rotated pixel back to the grid, we round to the nearest integers:\n",
    "\n",
    "$$\n",
    "\\text{new row} = \\operatorname{round}(-0.4142) = 0, \n",
    "\\quad\n",
    "\\text{new column} = \\operatorname{round}(1) = 1\n",
    "$$\n",
    "\n",
    "Thus, pixel $(0,0)$ maps to $(0,1)$ in the rotated image.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Do Gaps Appear?**\n",
    "\n",
    "When mapping all pixels:\n",
    "\n",
    "- **Overlaps**: Multiple original pixels may round to the same target coordinates.  \n",
    "- **Gaps**: Some target coordinates are never assigned, leaving empty pixels (white spaces).\n",
    "\n",
    "The rounding step in nearest-neighbor interpolation is the primary cause of these overlaps and gaps in the rotated image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4e: Bilinear Interpolation for Image Rotation\n",
    "\n",
    "**Task**: When rotating an image, gaps (white spaces) can appear due to nearest-neighbor assignment. To avoid these gaps, set each output pixel to a weighted average of the 4 nearest source pixels. This approach is called [bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation) and is common in image processing for producing smoother, gap-free results.\n",
    "\n",
    "**Steps**:\n",
    "1) For each output pixel:\n",
    "   - Translate its coordinates so that the rotation center is at the origin.\n",
    "   - Apply the inverse rotation (i.e., rotate backward by the desired angle).\n",
    "   - Translate the coordinates back to the original image space to locate the corresponding source position.\n",
    "2) If this source position falls outside the image, set the output pixel to 0.\n",
    "3) If the source position is inside the image:\n",
    "       - Find the four nearest source pixels surrounding this position (top-left, top-right, bottom-left, bottom-right).\n",
    "       - Compute the fractional distances from the source position to these neighbors (horizontal and vertical offsets).\n",
    "       - Compute a weighted average of the four neighbor values using these offsets (bilinear interpolation).\n",
    "4) Assign the computed value to the output pixel. If any neighbor used in the interpolation falls outside the image, treat its value as `0`.\n",
    "5) Repeat for all pixels. \n",
    "\n",
    "\n",
    "This method uses *inverse mapping* (sampling from the original image) rather than forward mapping (mapping source pixels to output), which helps prevent gaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_bilinear_rotation_matrix(theta, height=28, width=28):\n",
    "    \"\"\"\n",
    "    Create a (height*width, height*width) matrix that applies bilinear interpolation\n",
    "    for rotating a flattened image by theta degrees.\n",
    "    Each row of the matrix gives the weights for the input pixels that contribute to each output pixel.\n",
    "\n",
    "    Args:\n",
    "        theta (float): Angle of rotation in degrees.\n",
    "        height (int): Height of the image.\n",
    "        width (int): Width of the image.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (height*width, height*width) transformation matrix for rotating.\n",
    "    \"\"\"\n",
    "    theta = np.deg2rad(theta)\n",
    "    N = height * width\n",
    "    T = np.zeros((N, N))\n",
    "    center_i = (height - 1) / 2.0\n",
    "    center_j = (width - 1) / 2.0\n",
    "    ...\n",
    "    return T\n",
    "\n",
    "\n",
    "def rotate_image_bilinear(image, theta):\n",
    "    \"\"\"\n",
    "    Rotate an image using bilinear interpolation.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Flattened image array of shape (height*width,) or batch of images (N, height*width).\n",
    "        theta (float): Angle of rotation in degrees.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Rotated image as a flattened array.\n",
    "    \"\"\"\n",
    "    T = create_bilinear_rotation_matrix(theta)\n",
    "    return apply_transformation(image, T)\n",
    "    \n",
    "# rotate with matrix\n",
    "rotated = rotate_image(test_image, 45)\n",
    "rotated_interpolated = rotate_image_bilinear(test_image, 45)\n",
    "\n",
    "all_images = np.stack([test_image, rotated, rotated_interpolated])\n",
    "plot_labels = ['Original',  'Rotated 45°', 'Rotated 45° (Bilinear)']\n",
    "show_images(all_images, labels=plot_labels, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4f: Composing Transformations\n",
    "\n",
    "An advantage of transformation matrices is their composability: you can combine multiple transformations into a single matrix. This allows you to apply multiple transformations to an image with the same computational cost as applying just one.\n",
    "\n",
    "**Task**:\n",
    "\n",
    "1. **Compose Multiple Transformations**:\n",
    "    Implement `compose_transforms(*Ts)`, which takes any number of 784x784 transformation matrices (e.g., shift, rotate, blur) and returns a single matrix that represents applying all transformations in sequence. The transformations should be applied in the order they are provided: the first matrix is applied first, followed by the second, and so on.\n",
    "\n",
    "2. **Rotate and Blur**:\n",
    "    Implement `rotate_then_blur(image, theta, kernel_size)`, which rotates an image by `theta` degrees (without bilinear interpolation) and then applies a blur with a kernel of size `kernel_size`. Use `compose_transforms` to combine the transformations and apply them to the image.\n",
    "\n",
    "3. **Shift, Rotate, and Blur**:\n",
    "    Implement `shift_then_rotate_then_blur(image, dx, dy, theta, kernel_size)`, which shifts an image by `(dx, dy)`, rotates it by `theta` degrees (without bilinear interpolation), and then applies a blur with a kernel of size `kernel_size`. Again, use `compose_transforms` to combine the transformations and apply them to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compose_transforms(*Ts):\n",
    "  \"\"\"\n",
    "  Compose linear image transforms (each 784x784).\n",
    "  Inputs:\n",
    "    Ts: list of transformation matrices\n",
    "  Returns:\n",
    "    T_total: composition of all input transformations\n",
    "  \"\"\"\n",
    "  ...\n",
    "  return T_total\n",
    "\n",
    "def rotate_then_blur(image, theta, kernel_size):\n",
    "  \"\"\"\n",
    "  Rotate an image by theta degrees (without bilinear interpolation) and then blur it with a kernel of size kernel_size.\n",
    "  \"\"\"\n",
    "  ...\n",
    "\n",
    "def shift_then_rotate_then_blur(image, dx, dy, theta, kernel_size):\n",
    "  \"\"\"\n",
    "  Shift an image by (dx, dy), then rotate it by theta degrees (without bilinear interpolation), and then blur it with a kernel of size kernel_size.\n",
    "  \"\"\"\n",
    "  ...\n",
    "\n",
    "rotated_blurred_image = rotate_then_blur(test_image, 45, 3)\n",
    "shifted_rotated_blurred_image = shift_then_rotate_then_blur(test_image, 1, -4, 200, 5)\n",
    "\n",
    "all_images = np.stack([test_image, rotated_blurred_image, shifted_rotated_blurred_image])\n",
    "plot_labels = ['Original', 'Rotated 45° and Blurred 2x2', 'Shifted 5, Rotated 45° and Blurred 2x2']\n",
    "show_images(all_images, labels=plot_labels, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4g: Matrix Multiply Questions\n",
    " \n",
    "1. Does the order in which you apply transformations matter? Why or why not?  \n",
    "2. When can a transformation be undone (i.e., when can you multiply your augmented image by another transformation matrix to recover the original image)? What matrix would you multiply by to recover the original image?  \n",
    "3. Which of the augmentations implemented above can be \"undone\"? For augmentations that can be undone but may lose information (e.g., parts of the image are cut off), explain the conditions under which this occurs.  \n",
    "4. Which of these augmentations *cannot* be \"undone\" with another matrix multiplication? Why not?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Augmentation on Classifier Performance\n",
    "\n",
    "In this section, we will evaluate how our trained classifier performs on augmented versions of the test images. This will help us understand the robustness of the model to various transformations.\n",
    "\n",
    "The goal is to analyze the impact of different augmentation techniques on the classifier's performance. Specifically, we will:\n",
    "\n",
    "1. **Create Augmented Test Images**:\n",
    "    - Use the image augmentation functions (e.g., rotation, flipping, shifting, blurring) to generate transformed versions of the test images.\n",
    "\n",
    "2. **Evaluate the Classifier**:\n",
    "    - Test the classifier on the augmented images.\n",
    "    - Measure and compare the accuracy for each augmentation type.\n",
    "\n",
    "3. **Visualize Results**:\n",
    "    - Plot the performance metrics to identify which augmentations the classifier handles well and which ones degrade performance.\n",
    "\n",
    "### Problem 4h: Augmenting Test Images\n",
    "\n",
    "**Task**: Create augmented versions of the test images using the image augmentation functions we implemented earlier.\n",
    "\n",
    "**Steps**:\n",
    "1. Apply each augmentation technique (e.g., horizontal flip, vertical flip, rotation, shifting, blurring) to a sample of 100 test images. This should result in 1300 images (13 augmentations $\\times$ 100 test images)\n",
    "2. Store the augmented images in a structured format for evaluation.\n",
    "3. Ensure that the augmented images are labeled correctly for comparison with the classifier's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Test augmentation functions on a few examples\n",
    "test_images = np.stack(test_df['image'])\n",
    "test_labels = test_df['label']\n",
    "\n",
    "shift_inputs = [(5, 0), (-5, 0), (0, 5), (0, -5)]\n",
    "rotate_inputs = [45, 90, 200]\n",
    "blur_inputs = [3, 5]\n",
    "rotate_blur_inputs = [(45, 3), (90, 5)]\n",
    "shift_rotate_blur_inputs = [((5, 0), 45, 3), ((-5, 0), 90, 5)]\n",
    "\n",
    "augmented_data = []\n",
    "# Randomly sample 100 datapoints from test_images\n",
    "sample_idx = np.random.choice(len(test_images), 100, replace=False)\n",
    "test_images_sample = test_images[sample_idx]\n",
    "test_labels_sample = np.array(test_labels)[sample_idx]\n",
    "\n",
    "# TODO: Apply the augmentation functions we just created (shift, blur, rotate w/ bilinear, rotate then blur, shift then rotate then blur) to every image from test_images_sample\n",
    "# use the inputs defined above to apply the augmentations\n",
    "# Save the augmented images in a new DataFrame aug_df\n",
    "\n",
    "\n",
    "aug_df = ...\n",
    "\n",
    "# TODO: Select an image and visualize it with all the augmentations applied to it\n",
    "...\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4i: Evaluating Classifier Performance on Augmented Data\n",
    "\n",
    "**Task:** Evaluate the classifier's performance on the augmented test data and compare its accuracy across different types of augmentations. Create a `DataFrame` named `aug_performance` with the following columns:\n",
    "  - `augmentation`: A string describing the applied augmentation (e.g., \"shift_5_0\", \"rotate_90\", \"blur_2x2\").\n",
    "  - `accuracy`: The classifier's accuracy on the augmented data.\n",
    "  - `type`: The augmentation type (e.g., blur, rotate, shift, rotate_blur, shift_rotate_blur, none).\n",
    "\n",
    "**Hints:**\n",
    "1. Check the `image` column's data type and shape. The model likely expects a 3D array. Use `np.stack` to combine all augmented images in your `DataFrame` before scaling and passing them to the model.\n",
    "2. Use scikit-learn's `StandardScaler` to scale the data before evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "# Visualize performance: sort by accuracy, color by augmentation type (blur, rotate, shift, none)\n",
    "...\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4j: Analysis of Augmentation Techniques\n",
    "\n",
    "Among the augmentation techniques, which performed the best and which performed the worst? Why do you think this is the case? Provide reasoning based on the nature of the augmentations and their impact on the model's ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "You will being doing a LOT of matrix multiplication this semester, so get comfortable with these operations—they are fundamental to many machine learning algorithms you'll encounter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you submit, ensure save_models is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert save_models and load_saved_models, \"save_models and load_saved_models must be True\"\n",
    "\n",
    "assert os.path.exists('classifier.joblib'), \"classifier.joblib should exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gotten familiar with pandas, numpy, and the classic training loop let's look into how we can debug and improve classifiers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell if you are running the notebook in Google Colab to install the necessary dependencies, this may take a few minutes\n",
    "if IS_COLAB:\n",
    "    !apt-get install -y texlive texlive-xetex pandoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True, files=['classifier.joblib'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs189",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "require_no_pdf_confirmation": true,
   "tests": {
    "q0": {
     "name": "q0",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_columns = ['image', 'label']\n>>> assert all((col in df.columns for col in expected_columns)), f'Expected columns: {expected_columns}, got: {df.columns}'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert df['image'].apply(lambda x: isinstance(x, np.ndarray)).all(), f\"All 'image' values should be numpy arrays\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert df['label'].apply(lambda x: isinstance(x, str)).all(), \"All 'label' values should be strings\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert is_balanced\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> match = label_distribution_groupby.sort_index().equals(label_distribution.sort_index())\n>>> assert match, 'Label distributions do not match'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert examples.shape[0] == 20, f'Expected 20 samples, got {examples.shape[0]}'\n>>> counts = examples['label'].value_counts()\n>>> assert all(counts == 2), f'Each class should have 2 samples, got counts: {counts.to_dict()}'\n>>> assert set(examples.columns) == set(df.columns), f'Expected columns {set(df.columns)}, got {set(examples.columns)}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert df['image'].apply(lambda img: img.shape == (784,)).all(), 'Not all images are flattened to shape (784,)'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert len(kmeans_df) == len(df_sample), f'kmeans_df and df have different lengths {len(kmeans_df)} != {len(df)}'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert set(kmeans_df.columns) == set(['cluster', 'label', 'image']), f'kmeans_df has unexpected columns {set(kmeans_df.columns)}'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> acc = model.score(X_test_sc, y_test)\n>>> assert acc < 0.89 and acc > 0.88, f'Accuracy does not match expected ({int(acc * 100)}%)'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> columns = ['image', 'label', 'predicted_label', 'correct', 'probs', 'confidence']\n>>> assert all((col in train_df.columns for col in columns)), f'train_df missing columns: {set(columns) - set(train_df.columns)}'\n>>> assert all((col in test_df.columns for col in columns)), f'test_df missing columns: {set(columns) - set(test_df.columns)}'\n>>> first = train_df.iloc[0]\n>>> assert type(first['image']).__name__ == 'ndarray' and getattr(first['image'], 'shape', None) == (784,), f\"image: {type(first['image'])}, shape: {getattr(first['image'], 'shape', None)}\"\n>>> assert isinstance(first['label'], str), f\"label: {type(first['label'])}\"\n>>> assert isinstance(first['predicted_label'], str), f\"predicted_label: {type(first['predicted_label'])}\"\n>>> assert type(first['correct']).__name__ in ['bool', 'bool_', 'bool8', 'bool_'], f\"correct: {type(first['correct'])}\"\n>>> assert type(first['probs']).__name__ == 'list', f\"probs: {type(first['probs'])}\"\n>>> assert type(first['confidence']).__name__ in ['float64', 'float'], f\"confidence: {type(first['confidence'])}\"\n>>> assert all((isinstance(p, list) and len(p) == 10 for p in train_df['probs'])), \"'probs' in train_df not correct shape (should be length 10)\"\n>>> assert all((isinstance(p, list) and len(p) == 10 for p in test_df['probs'])), \"'probs' in test_df not correct shape (should be length 10)\"\n>>> assert all((abs(conf - max(probs)) < 1e-06 for conf, probs in zip(train_df['confidence'], train_df['probs']))), \"'confidence' in train_df not max of 'probs'\"\n>>> assert all((abs(conf - max(probs)) < 1e-06 for conf, probs in zip(test_df['confidence'], test_df['probs']))), \"'confidence' in test_df not max of 'probs'\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert set(class_accuracy.columns) == {'split', 'label', 'correct'}, f'class_accuracy has incorrect columns: {set(class_accuracy.columns)}'\n>>> assert set(class_accuracy['split']) == {'train', 'test'}, f\"class_accuracy has incorrect splits: {set(class_accuracy['split'])}\"\n>>> all_labels = set(train_df['label']).union(set(test_df['label']))\n>>> assert set(class_accuracy['label']) == all_labels, f\"class_accuracy has incorrect labels: {set(class_accuracy['label'])}\"\n>>> assert class_accuracy['correct'].between(0, 1).all(), \"class_accuracy has incorrect 'correct' values (should be between 0 and 1)\"\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert accuracy_from_matrix is not None, 'accuracy_from_matrix variable should exist'\n>>> assert isinstance(accuracy_from_matrix, float), 'accuracy_from_matrix should be a float'\n>>> assert accuracy_from_matrix == test_df['correct'].mean(), f\"Accuracy from confusion matrix {accuracy_from_matrix} does not match accuracy from DataFrame {test_df['correct'].mean()}\"\n>>> assert per_class_metrics is not None, 'per_class_metrics variable should exist'\n>>> assert isinstance(per_class_metrics, list), 'per_class_metrics should be a list'\n>>> assert len(per_class_metrics) == len(class_names), 'per_class_metrics should have the same length as class_names'\n>>> assert all((isinstance(metric, dict) for metric in per_class_metrics)), 'all elements of per_class_metrics should be dictionaries'\n>>> dict_keys = ['class', 'precision', 'recall']\n>>> assert all((set(metric.keys()) == set(dict_keys) for metric in per_class_metrics)), f'all dictionaries in per_class_metrics should have the keys {dict_keys}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3f": {
     "name": "q3f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'least_confident' in locals(), 'least_confident variable should exist'\n>>> assert isinstance(least_confident, pd.DataFrame), 'least_confident should be a pandas DataFrame'\n>>> assert (least_confident['confidence'].diff()[1:] >= 0).all(), 'least_confident should be sorted by confidence in ascending order'\n>>> assert len(least_confident) == len(test_df), 'least_confident should contain all rows from test_df'\n>>> assert set(least_confident.columns) == set(test_df.columns), 'least_confident should have the same columns as test_df'\n>>> assert least_confident.iloc[0]['confidence'] == test_df['confidence'].min(), 'First row should have lowest confidence'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3g": {
     "name": "q3g",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'test_df_boot' in locals(), 'test_df_boot should exist'\n>>> assert all(test_df_boot['label'] == 'Ankle boot'), \"test_df_boot should only contain rows where label is 'Ankle boot'\"\n>>> assert all(test_df_boot['predicted_label'] == 'Ankle boot'), \"test_df_boot should only contain rows where predicted_label is 'Ankle boot'\"\n>>> assert 'low_conf_correct' in locals(), 'low_conf_correct should exist'\n>>> assert all(low_conf_correct['correct'] == True), 'low_conf_correct should only contain correctly classified examples'\n>>> assert low_conf_correct.iloc[0]['confidence'] <= low_conf_correct.iloc[-1]['confidence'], 'low_conf_correct should be sorted by confidence in ascending order'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3i": {
     "name": "q3i",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'test_df_trouser' in locals(), 'test_df_trouser should exist'\n>>> assert all(test_df_trouser['label'] == 'Trouser'), \"test_df_trouser should only contain rows where label is 'Trouser'\"\n>>> assert all(high_conf_incorrect['correct'] == False), 'high_conf_incorrect should only contain incorrect predictions'\n>>> assert 'high_conf_incorrect' in locals(), 'high_conf_incorrect should exist'\n>>> assert all(high_conf_incorrect['confidence'].diff().fillna(0) <= 0), 'high_conf_incorrect should be sorted by confidence in descending order'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert create_horizontal_flip_matrix().shape == (784, 784), 'Horizontal flip matrix should be 784x784'\n>>> gt_horizontal_flip_image = np.load('public_solutions/horizontal_flip_image.npy')\n>>> assert np.array_equal(horizontal_flip(test_image), gt_horizontal_flip_image), 'Horizontal flip image does not match solution'\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert create_shift_matrix(5, 0).shape == (784, 784), 'Shift matrix should be 784x784'\n>>> gt_shift_left_transform = np.load('public_solutions/shift_left_transform.npy')\n>>> gt_shift_right_transform = np.load('public_solutions/shift_right_transform.npy')\n>>> gt_shift_up_transform = np.load('public_solutions/shift_up_transform.npy')\n>>> gt_shift_down_transform = np.load('public_solutions/shift_down_transform.npy')\n>>> assert np.array_equal(shift_image(test_image, 5, 0), gt_shift_right_transform), 'Shift right image does not match solution'\n>>> assert np.array_equal(shift_image(test_image, -5, 0), gt_shift_left_transform), 'Shift left image does not match solution'\n>>> assert np.array_equal(shift_image(test_image, 0, 5), gt_shift_up_transform), 'Shift up image does not match solution'\n>>> assert np.array_equal(shift_image(test_image, 0, -5), gt_shift_down_transform), 'Shift down image does not match solution'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert create_blur_matrix(2).shape == (784, 784), 'Blur matrix should be 784x784'\n>>> gt_blur_3x3_transform = np.load('public_solutions/blur_3x3_transform.npy')\n>>> gt_blur_5x5_transform = np.load('public_solutions/blur_5x5_transform.npy')\n>>> gt_blur_3x3_transform_updated = np.load('public_solutions/blur_3x3_transform_updated.npy')\n>>> gt_blur_5x5_transform_updated = np.load('public_solutions/blur_5x5_transform_updated.npy')\n>>> assert np.allclose(blur_image(test_image, 3), gt_blur_3x3_transform, rtol=1e-05, atol=1e-08) or np.allclose(blur_image(test_image, 3), gt_blur_3x3_transform_updated, rtol=1e-05, atol=1e-08), 'Blur 3x3 image does not match solution'\n>>> assert np.allclose(blur_image(test_image, 5), gt_blur_5x5_transform, rtol=1e-05, atol=1e-08) or np.allclose(blur_image(test_image, 5), gt_blur_5x5_transform_updated, rtol=1e-05, atol=1e-08), 'Blur 5x5 image does not match solution'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert create_rotation_matrix(15).shape == (784, 784), 'Rotation matrix should be 784x784'\n>>> gt_rotate_45_transform = np.load('public_solutions/rotate_45_transform.npy')\n>>> gt_rotate_90_transform = np.load('public_solutions/rotate_90_transform.npy')\n>>> gt_rotate_200_transform = np.load('public_solutions/rotate_200_transform.npy')\n>>> gt_rotate_270_transform = np.load('public_solutions/rotate_270_transform.npy')\n>>> gt_rotate_45_transform_updated = np.load('public_solutions/rotate_45_transform_updated.npy')\n>>> gt_rotate_90_transform_updated = np.load('public_solutions/rotate_90_transform_updated.npy')\n>>> gt_rotate_200_transform_updated = np.load('public_solutions/rotate_200_transform_updated.npy')\n>>> gt_rotate_270_transform_updated = np.load('public_solutions/rotate_270_transform_updated.npy')\n>>> assert np.array_equal(rotate_image(test_image, 45), gt_rotate_45_transform) or np.array_equal(rotate_image(test_image, 45), gt_rotate_45_transform_updated), 'Rotate 45 image does not match solution'\n>>> assert np.array_equal(rotate_image(test_image, 90), gt_rotate_90_transform) or np.array_equal(rotate_image(test_image, 90), gt_rotate_90_transform_updated), 'Rotate 90 image does not match solution'\n>>> assert np.array_equal(rotate_image(test_image, 200), gt_rotate_200_transform) or np.array_equal(rotate_image(test_image, 200), gt_rotate_200_transform_updated), 'Rotate 200 image does not match solution'\n>>> assert np.array_equal(rotate_image(test_image, 270), gt_rotate_270_transform) or np.array_equal(rotate_image(test_image, 270), gt_rotate_270_transform_updated), 'Rotate 270 image does not match solution'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4e": {
     "name": "q4e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert create_bilinear_rotation_matrix(15).shape == (784, 784), 'Rotation matrix should be 784x784'\n>>> gt_rotate_bilinear_45_image = np.load('public_solutions/rotate_45_bilinear_transform.npy')\n>>> gt_rotate_bilinear_45_image_updated = np.load('public_solutions/rotate_45_bilinear_transform_updated.npy')\n>>> assert np.allclose(rotate_image_bilinear(test_image, 45), gt_rotate_bilinear_45_image, rtol=1e-05, atol=1e-08) or np.allclose(rotate_image_bilinear(test_image, 45), gt_rotate_bilinear_45_image_updated, rtol=1e-05, atol=1e-08), 'Rotate bilinear 45 image does not match solution'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4f": {
     "name": "q4f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert compose_transforms(create_rotation_matrix(45), create_blur_matrix(2)).shape == (784, 784), 'Compose transforms should return a 784x784 matrix'\n>>> gt_rotate_then_blur_transform = np.load('public_solutions/rotate_then_blur_transform.npy')\n>>> gt_shift_then_rotate_then_blur_transform = np.load('public_solutions/shift_then_rotate_then_blur_transform.npy')\n>>> gt_rotate_then_blur_transform_updated = np.load('public_solutions/rotate_then_blur_transform_updated.npy')\n>>> gt_shift_then_rotate_then_blur_transform_updated = np.load('public_solutions/shift_then_rotate_then_blur_transform_updated.npy')\n>>> assert np.array_equal(rotate_then_blur(test_image, 45, 2), gt_rotate_then_blur_transform) or np.array_equal(rotate_then_blur(test_image, 45, 3), gt_rotate_then_blur_transform_updated), 'Rotate then blur image does not match solution'\n>>> assert np.allclose(shift_then_rotate_then_blur(test_image, 1, -4, 200, 3), gt_shift_then_rotate_then_blur_transform, rtol=1e-05, atol=1e-08) or np.allclose(shift_then_rotate_then_blur(test_image, 1, -4, 200, 5), gt_shift_then_rotate_then_blur_transform_updated, rtol=1e-05, atol=1e-08), 'Shift then rotate then blur image does not match solution'\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4h": {
     "name": "q4h",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(augmented_data[0], dict), 'Each item in augmented_data should be a dictionary'\n>>> assert all((key in augmented_data[0] for key in ['original_idx', 'augmentation', 'image', 'label'])), 'Augmented data dictionaries missing required keys'\n>>> aug_types = aug_df['augmentation'].unique()\n>>> assert any(('shift' in aug for aug in aug_types)), 'No shift augmentations found'\n>>> assert any(('rotate' in aug for aug in aug_types)), 'No rotation augmentations found'\n>>> assert any(('blur' in aug for aug in aug_types)), 'No blur augmentations found'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4i": {
     "name": "q4i",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'aug_performance' in locals(), 'aug_performance DataFrame not found. Did you create it using groupby and agg operations?'\n>>> required_columns = {'augmentation', 'accuracy', 'type'}\n>>> assert required_columns.issubset(set(aug_performance.columns)), \"aug_performance should have columns: 'augmentation', 'accuracy', and 'type'\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
